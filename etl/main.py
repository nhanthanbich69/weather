# ============================================================
# WEATHER DATA AUTO-UPDATER ‚Äî FULL VI·ªÜT NAM (63 T·ªàNH)
# - Gom 63 t·ªânh v√†o 1 file: Th·ªùi_ti·∫øt_Vi·ªát_Nam.csv
# - Anti-429 ki·ªÉu RATE_LIMIT_HARD:
#     + Retry nhi·ªÅu l·∫ßn, ch·ªù l√¢u
#     + N·∫øu v·∫´n 429 -> d·ª´ng ch∆∞∆°ng tr√¨nh, l∆∞u file hi·ªán t·∫°i
#     + L·∫ßn sau ch·∫°y l·∫°i: t·ª± crawl ti·∫øp cho t·ª´ng t·ªânh (theo Tinh_thanh)
# ============================================================

import requests, pandas as pd, numpy as np, time, random, os, re
from datetime import datetime, timedelta, date
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from geopy.geocoders import Nominatim

# =============================
# 1. FOLDER L∆ØU FILE
# =============================
# Use project-relative paths instead of hardcoded absolute paths
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(script_dir)

# Define data directories
data_lakehouse_dir = os.path.join(project_root, "data", "data lakehouse")
location_dir = os.path.join(project_root, "data", "location")

os.makedirs(data_lakehouse_dir, exist_ok=True)
os.makedirs(location_dir, exist_ok=True)

loc_path = os.path.join(location_dir, "vn_locations.csv")
out_vn_path = os.path.join(data_lakehouse_dir, "data.csv")

# =============================
# 2. DANH S√ÅCH 63 T·ªàNH
# =============================
provinces = [
    "An Giang","B·∫Øc Giang","B·∫Øc K·∫°n","B·∫°c Li√™u","B·∫Øc Ninh","B·∫øn Tre","B√¨nh ƒê·ªãnh",
    "B√¨nh D∆∞∆°ng","B√¨nh Thu·∫≠n","B√¨nh Ph∆∞·ªõc","C√† Mau","C·∫ßn Th∆°","Cao B·∫±ng","ƒê√† N·∫µng",
    "ƒê·∫Øk L·∫Øk","ƒê·∫Øk N√¥ng","ƒêi·ªán Bi√™n","ƒê·ªìng Nai","ƒê·ªìng Th√°p","Gia Lai","H√† Giang",
    "H√† Nam","H√† N·ªôi","H√† Tƒ©nh","H·∫£i D∆∞∆°ng","H·∫£i Ph√≤ng","H·∫≠u Giang","Ho√† B√¨nh",
    "Th·ª´a Thi√™n Hu·∫ø","H∆∞ng Y√™n","Kh√°nh Ho√†","Ki√™n Giang","Kon Tum","Lai Ch√¢u",
    "L√¢m ƒê·ªìng","L·∫°ng S∆°n","L√†o Cai","Long An","Nam ƒê·ªãnh","Ngh·ªá An","Ninh B√¨nh",
    "Ninh Thu·∫≠n","Ph√∫ Th·ªç","Ph√∫ Y√™n","Qu·∫£ng B√¨nh","Qu·∫£ng Nam","Qu·∫£ng Ng√£i",
    "Qu·∫£ng Ninh","Qu·∫£ng Tr·ªã","S√≥c TrƒÉng","S∆°n La","T√¢y Ninh","Th√°i B√¨nh",
    "Th√°i Nguy√™n","Thanh Ho√°","Ti·ªÅn Giang","H·ªì Ch√≠ Minh","Tr√† Vinh",
    "Tuy√™n Quang","Vƒ©nh Long","Vƒ©nh Ph√∫c","B√† R·ªãa - V≈©ng T√†u","Y√™n B√°i"
]

# =============================
# 3. T·∫†O FILE T·ªåA ƒê·ªò (CH·ªà CH·∫†Y 1 L·∫¶N)
# =============================
if not os.path.exists(loc_path):
    print("ƒêang t·∫°o file t·ªça ƒë·ªô (ch·∫°y 1 l·∫ßn)...")
    geolocator = Nominatim(user_agent="vn_weather_locator")
    rows = []

    for p in provinces:
        try:
            loc = geolocator.geocode(f"{p}, Vietnam", timeout=10)
            if loc:
                print(f"  ‚úî {p}: {loc.latitude:.4f}, {loc.longitude:.4f}")
                rows.append({
                    "Tinh_thanh": p,
                    "lat": round(loc.latitude, 4),
                    "lon": round(loc.longitude, 4)
                })
            else:
                print(f"  ‚úñ Kh√¥ng t√¨m ƒë∆∞·ª£c: {p}")
            time.sleep(random.uniform(1.5, 3.0))
        except Exception as e:
            print(f"  ‚úñ L·ªói geocode {p}: {e}")
            time.sleep(random.uniform(3.0, 5.0))

    pd.DataFrame(rows).to_csv(loc_path, index=False, encoding="utf-8-sig")
    print(f"‚úî ƒê√£ l∆∞u file t·ªça ƒë·ªô: {loc_path}")

# =============================
# 4. LOAD T·ªåA ƒê·ªò
# =============================
loc_df = pd.read_csv(loc_path)

# =============================
# 5. MAPPING C·ªòT HOURLY + DAILY (TI·∫æNG VI·ªÜT, C√ì D·∫§U)
# =============================

COLUMN_MAP = {
    "temperature_2m": "Nhi·ªát ƒë·ªô (¬∞C)",
    "relative_humidity_2m": "ƒê·ªô ·∫©m t∆∞∆°ng ƒë·ªëi (%)",
    "dew_point_2m": "ƒêi·ªÉm s∆∞∆°ng (¬∞C)",
    "apparent_temperature": "Nhi·ªát ƒë·ªô c·∫£m nh·∫≠n (¬∞C)",
    "pressure_msl": "√Åp su·∫•t m·ª±c bi·ªÉn (hPa)",
    "surface_pressure": "√Åp su·∫•t b·ªÅ m·∫∑t (hPa)",
    "precipitation": "L∆∞·ª£ng m∆∞a (mm)",
    "cloud_cover": "ƒê·ªô ph·ªß m√¢y (%)",
    "cloud_cover_low": "M√¢y th·∫•p (%)",
    "cloud_cover_mid": "M√¢y trung (%)",
    "cloud_cover_high": "M√¢y cao (%)",
    "wind_speed_10m": "T·ªëc ƒë·ªô gi√≥ 10m (m/s)",
    "wind_speed_100m": "T·ªëc ƒë·ªô gi√≥ 100m (m/s)",
    "wind_direction_10m": "H∆∞·ªõng gi√≥ 10m (¬∞)",
    "wind_direction_100m": "H∆∞·ªõng gi√≥ 100m (¬∞)",
    "wind_gusts_10m": "T·ªëc ƒë·ªô gi√≥ gi·∫≠t 10m (m/s)",
    "weather_code": "M√£ th·ªùi ti·∫øt",
    "shortwave_radiation": "B·ª©c x·∫° ng·∫Øn (W/m2)",
    "sunshine_duration": "Th·ªùi gian n·∫Øng (s)",
    "et0_fao_evapotranspiration": "B·ªëc h∆°i tham chi·∫øu FAO gi·ªù (mm)",
    "vapour_pressure_deficit": "Thi·∫øu h·ª•t √°p su·∫•t h∆°i (kPa)"
}

DAILY_COLUMN_MAP = {
    "weather_code": "M√£ th·ªùi ti·∫øt ng√†y",
    "temperature_2m_max": "Nhi·ªát ƒë·ªô t·ªëi ƒëa ng√†y (¬∞C)",
    "temperature_2m_min": "Nhi·ªát ƒë·ªô t·ªëi thi·ªÉu ng√†y (¬∞C)",
    "temperature_2m_mean": "Nhi·ªát ƒë·ªô trung b√¨nh ng√†y (¬∞C)",
    "apparent_temperature_max": "Nhi·ªát ƒë·ªô c·∫£m nh·∫≠n t·ªëi ƒëa ng√†y (¬∞C)",
    "apparent_temperature_min": "Nhi·ªát ƒë·ªô c·∫£m nh·∫≠n t·ªëi thi·ªÉu ng√†y (¬∞C)",
    "apparent_temperature_mean": "Nhi·ªát ƒë·ªô c·∫£m nh·∫≠n trung b√¨nh ng√†y (¬∞C)",
    "precipitation_sum": "T·ªïng l∆∞·ª£ng m∆∞a ng√†y (mm)",
    "precipitation_hours": "S·ªë gi·ªù c√≥ m∆∞a (h)",
    "daylight_duration": "Th·ªùi l∆∞·ª£ng ban ng√†y (s)",
    "sunshine_duration": "T·ªïng th·ªùi gian n·∫Øng ng√†y (s)",
    "shortwave_radiation_sum": "T·ªïng b·ª©c x·∫° ng·∫Øn ng√†y (W/m2)",
    "wind_speed_10m_max": "T·ªëc ƒë·ªô gi√≥ 10m t·ªëi ƒëa ng√†y (m/s)",
    "wind_gusts_10m_max": "T·ªëc ƒë·ªô gi√≥ gi·∫≠t 10m t·ªëi ƒëa ng√†y (m/s)",
    "wind_direction_10m_dominant": "H∆∞·ªõng gi√≥ ∆∞u th·∫ø 10m (¬∞)",
    "relative_humidity_2m_mean": "ƒê·ªô ·∫©m trung b√¨nh ng√†y (%)",
    "dew_point_2m_mean": "ƒêi·ªÉm s∆∞∆°ng trung b√¨nh ng√†y (¬∞C)",
    "cloud_cover_mean": "M√¢y trung b√¨nh (%)",
    "surface_pressure_mean": "√Åp su·∫•t b·ªÅ m·∫∑t trung b√¨nh (hPa)",
    "et0_fao_evapotranspiration": "B·ªëc h∆°i tham chi·∫øu FAO ng√†y (mm)",
    "sunrise": "Gi·ªù m·∫∑t tr·ªùi m·ªçc",
    "sunset": "Gi·ªù m·∫∑t tr·ªùi l·∫∑n"
}

# =============================
# 6. SESSION CHUNG (D√ôNG L·∫†I K·∫æT N·ªêI)
# =============================
def make_session():
    session = requests.Session()
    retry_cfg = Retry(
        total=3,
        backoff_factor=2,
        status_forcelist=[500, 502, 503, 504],
        raise_on_status=False,
    )
    adapter = HTTPAdapter(max_retries=retry_cfg)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    return session

session_global = make_session()

# =============================
# 7. H√ÄM FETCH 1 BLOCK V·ªöI RATE_LIMIT_HARD
# =============================
def fetch_block(lat, lon, start_str, end_str, tinh_thanh,
                max_attempts=6, base_wait_429=60):
    """
    L·∫•y d·ªØ li·ªáu 1 block (t·ªëi ƒëa ~365 ng√†y) cho 1 t·ªânh.
    - G·∫∑p 429: ch·ªù l√¢u + retry nhi·ªÅu l·∫ßn.
    - N·∫øu v·∫´n 429 sau max_attempts -> raise RuntimeError("RATE_LIMIT_HARD").
    - G·∫∑p l·ªói m·∫°ng: retry c√≥ backoff.
    - Sau merge: Gi·ªù m·∫∑t tr·ªùi m·ªçc / l·∫∑n ch·ªâ gi·ªØ HH:MM.
    - S·∫Øp x·∫øp c·ªôt: Tinh_thanh | M√£ th·ªùi ti·∫øt | M√£ th·ªùi ti·∫øt ng√†y | ... | Datetime
    """
    url = "https://archive-api.open-meteo.com/v1/archive"

    params = {
        "latitude": lat,
        "longitude": lon,
        "start_date": start_str,
        "end_date": end_str,
        "hourly": ",".join(COLUMN_MAP.keys()),
        "daily": ",".join(DAILY_COLUMN_MAP.keys()),
        "timezone": "Asia/Ho_Chi_Minh",
        "pressure_unit": "hPa",
        "temperature_unit": "celsius",
        "wind_speed_unit": "ms"
    }

    for attempt in range(1, max_attempts + 1):
        try:
            r = session_global.get(url, params=params, timeout=120)
        except Exception as e:
            wait = 10 * attempt
            print(f"    ‚ö† L·ªói k·∫øt n·ªëi (attempt {attempt}/{max_attempts}): {e} ‚Äì ngh·ªâ {wait}s")
            time.sleep(wait)
            continue

        status = r.status_code

        # === 200 OK ===
        if status == 200:
            data = r.json()

            # HOURLY
            df_h = pd.DataFrame(data["hourly"])
            df_h["Datetime"] = pd.to_datetime(df_h["time"], errors="coerce")
            df_h.drop(columns=["time"], inplace=True)
            df_h.rename(columns=COLUMN_MAP, inplace=True)
            df_h["Tinh_thanh"] = tinh_thanh
            df_h["Date"] = df_h["Datetime"].dt.date

            # DAILY
            df_d = pd.DataFrame(data["daily"])
            df_d["Date"] = pd.to_datetime(df_d["time"], errors="coerce").dt.date
            df_d.drop(columns=["time"], inplace=True)
            df_d.rename(columns=DAILY_COLUMN_MAP, inplace=True)

            if "Gi·ªù m·∫∑t tr·ªùi m·ªçc" in df_d.columns:
                df_d["Gi·ªù m·∫∑t tr·ªùi m·ªçc"] = pd.to_datetime(
                    df_d["Gi·ªù m·∫∑t tr·ªùi m·ªçc"], errors="coerce"
                ).dt.strftime("%H:%M")

            if "Gi·ªù m·∫∑t tr·ªùi l·∫∑n" in df_d.columns:
                df_d["Gi·ªù m·∫∑t tr·ªùi l·∫∑n"] = pd.to_datetime(
                    df_d["Gi·ªù m·∫∑t tr·ªùi l·∫∑n"], errors="coerce"
                ).dt.strftime("%H:%M")

            # MERGE HOURLY + DAILY THEO NG√ÄY
            df = df_h.merge(df_d, on="Date", how="left")
            df.drop(columns=["Date"], inplace=True)

            # S·∫ÆP X·∫æP C·ªòT:
            # - Tinh_thanh ƒë·∫ßu
            # - Sau ƒë√≥: M√£ th·ªùi ti·∫øt, M√£ th·ªùi ti·∫øt ng√†y (n·∫øu c√≥)
            # - C√°c c·ªôt c√≤n l·∫°i
            # - Datetime cu·ªëi
            special_cols = ["M√£ th·ªùi ti·∫øt", "M√£ th·ªùi ti·∫øt ng√†y"]

            cols = ["Tinh_thanh"]
            for sc in special_cols:
                if sc in df.columns:
                    cols.append(sc)

            middle_cols = [
                c for c in df.columns
                if c not in (["Tinh_thanh", "Datetime"] + special_cols)
            ]

            cols += middle_cols
            cols.append("Datetime")
            df = df[cols]

            return df

        # === 429 RATE LIMIT ===
        if status == 429:
            wait = base_wait_429 * attempt + random.randint(0, 60)
            print(f"    ‚ö† 429 (rate limit) attempt {attempt}/{max_attempts} ‚Äì ngh·ªâ {wait}s r·ªìi th·ª≠ l·∫°i...")
            time.sleep(wait)
            continue

        # === 5xx L·ªñI SERVER T·∫†M ===
        if status in (500, 502, 503, 504):
            wait = 20 * attempt
            print(f"    ‚ö† L·ªói server {status} attempt {attempt}/{max_attempts} ‚Äì ngh·ªâ {wait}s r·ªìi th·ª≠ l·∫°i...")
            time.sleep(wait)
            continue

        # === L·ªñI KH√ÅC (400, 403, ...) -> B·ªé BLOCK ===
        print(f"    ‚ùå L·ªói API {status} cho block {start_str} ‚Üí {end_str}, b·ªè block.")
        return None

    # N·∫øu t·ªõi ƒë√¢y: 429 / 5xx li√™n t·ª•c -> RATE_LIMIT_HARD
    print(f"    ‚ùå Th·ª≠ {max_attempts} l·∫ßn v·∫´n l·ªói (429/5xx). RATE_LIMIT_HARD t·∫°i block {start_str} ‚Üí {end_str}.")
    raise RuntimeError("RATE_LIMIT_HARD")

# =============================
# 8. MAIN ‚Äì CRAWL TO√ÄN B·ªò VI·ªÜT NAM
# =============================
start_all = date(2000, 1, 1)
today = datetime.now().date()
yesterday = today - timedelta(days=1)

# Load file t·ªïng n·∫øu ƒë√£ t·ªìn t·∫°i
if os.path.exists(out_vn_path):
    df_vn = pd.read_csv(out_vn_path)
    if "Datetime" in df_vn.columns:
        df_vn["Datetime"] = pd.to_datetime(df_vn["Datetime"], errors="coerce")
        df_vn.dropna(subset=["Datetime"], inplace=True)
        df_vn.sort_values(["Tinh_thanh", "Datetime"], inplace=True)
        print(f"ƒê√£ c√≥ file t·ªïng: {out_vn_path}")
    else:
        print("File t·ªïng kh√¥ng c√≥ c·ªôt Datetime, crawl l·∫°i t·ª´ 2000.")
        df_vn = pd.DataFrame()
else:
    print("Ch∆∞a c√≥ file t·ªïng ‚Äî crawl t·ª´ 2000-01-01.")
    df_vn = pd.DataFrame()

print("=== B·∫ÆT ƒê·∫¶U CRAWL TO√ÄN B·ªò VI·ªÜT NAM ===")

try:
    # Loop t·ª´ng t·ªânh
    for idx, row in loc_df.iterrows():
        tinh = row["Tinh_thanh"]
        lat, lon = row["lat"], row["lon"]

        print(f"\n({idx+1}/{len(loc_df)}) T·ªânh: {tinh} ({lat}, {lon})")

        # X√°c ƒë·ªãnh ng√†y b·∫Øt ƒë·∫ßu cho ri√™ng t·ªânh n√†y (d·ª±a tr√™n file t·ªïng hi·ªán c√≥)
        if not df_vn.empty and tinh in df_vn["Tinh_thanh"].unique():
            df_tinh = df_vn[df_vn["Tinh_thanh"] == tinh].copy()
            df_tinh["Datetime"] = pd.to_datetime(df_tinh["Datetime"], errors="coerce")
            df_tinh.dropna(subset=["Datetime"], inplace=True)

            if not df_tinh.empty:
                last_date_tinh = df_tinh["Datetime"].max().date()
                start_date = last_date_tinh + timedelta(days=1)
                print(f"  ƒê√£ c√≥ d·ªØ li·ªáu {tinh} t·ªõi: {last_date_tinh}")
            else:
                start_date = start_all
        else:
            start_date = start_all

        # N·∫øu ƒë√£ ƒë·ªß t·ªõi h√¥m qua th√¨ b·ªè qua
        if start_date > yesterday:
            print(f"  {tinh} ƒë√£ ƒë·ªß d·ªØ li·ªáu t·ªõi h√¥m qua ({yesterday}), b·ªè qua.")
            continue

        print(f"  S·∫Ω crawl th√™m cho {tinh} t·ª´ {start_date} ‚Üí {yesterday}")

        df_blocks = []
        cur = start_date

        try:
            while cur <= yesterday:
                # Block 365 ng√†y (1 nƒÉm) ƒë·ªÉ gi·∫£m 429
                block_end = min(cur + timedelta(days=365), yesterday)
                s_str = cur.strftime("%Y-%m-%d")
                e_str = block_end.strftime("%Y-%m-%d")

                print(f"  ‚Üí Block {s_str} ‚Üí {e_str}")
                df_blk = fetch_block(lat, lon, s_str, e_str, tinh)

                if df_blk is not None and not df_blk.empty:
                    df_blocks.append(df_blk)
                    print(f"    ‚úî L·∫•y ƒë∆∞·ª£c {len(df_blk)} d√≤ng")
                else:
                    print("    ‚úñ Block n√†y kh√¥ng c√≥ d·ªØ li·ªáu / b·ªã b·ªè qua")

                cur = block_end + timedelta(days=1)
                # delay gi·ªØa c√°c block
                time.sleep(random.uniform(5, 10))

        except RuntimeError as e:
            # RATE_LIMIT_HARD: d·ª´ng to√†n b·ªô, nh∆∞ng ph·∫£i l∆∞u file tr∆∞·ªõc
            if "RATE_LIMIT_HARD" in str(e):
                print("  ‚õî G·∫∑p RATE_LIMIT_HARD (429 li√™n t·ª•c). D·ª™NG TO√ÄN B·ªò CH∆Ø∆†NG TR√åNH.")
                # G·ªôp nh·ªØng block ƒë√£ l·∫•y ƒë∆∞·ª£c cho t·ªânh hi·ªán t·∫°i (n·∫øu c√≥)
                if df_blocks:
                    df_new_tinh = pd.concat(df_blocks, ignore_index=True)
                    if df_vn.empty:
                        df_vn = df_new_tinh
                    else:
                        df_vn = pd.concat([df_vn, df_new_tinh], ignore_index=True)

                # L√†m s·∫°ch + l∆∞u file t·ªïng tr∆∞·ªõc khi d·ª´ng
                if not df_vn.empty:
                    df_vn.drop_duplicates(subset=["Tinh_thanh", "Datetime"], inplace=True)
                    df_vn["Datetime"] = pd.to_datetime(df_vn["Datetime"], errors="coerce")
                    df_vn.dropna(subset=["Datetime"], inplace=True)
                    df_vn.sort_values(["Tinh_thanh", "Datetime"], inplace=True)

                    # S·∫Øp x·∫øp l·∫°i c·ªôt: Tinh_thanh | M√£ th·ªùi ti·∫øt | M√£ th·ªùi ti·∫øt ng√†y | ... | Datetime
                    special_cols = ["M√£ th·ªùi ti·∫øt", "M√£ th·ªùi ti·∫øt ng√†y"]
                    cols = ["Tinh_thanh"]
                    for sc in special_cols:
                        if sc in df_vn.columns:
                            cols.append(sc)

                    middle_cols = [
                        c for c in df_vn.columns
                        if c not in (["Tinh_thanh", "Datetime"] + special_cols)
                    ]

                    cols += middle_cols
                    cols.append("Datetime")
                    df_vn = df_vn[cols]

                    df_vn.to_csv(out_vn_path, index=False, encoding="utf-8-sig")
                    print(f"  ‚úî ƒê√É L∆ØU T·∫†M FILE T·ªîNG TR∆Ø·ªöC KHI D·ª™NG: {out_vn_path}")
                raise
            else:
                raise

        # H·∫øt while, kh√¥ng c√≥ RATE_LIMIT_HARD -> g·ªôp d·ªØ li·ªáu t·ªânh n√†y v√†o file t·ªïng
        if df_blocks:
            df_new = pd.concat(df_blocks, ignore_index=True)

            if df_vn.empty:
                df_vn = df_new
            else:
                df_vn = pd.concat([df_vn, df_new], ignore_index=True)

            # L√†m s·∫°ch + sort + Datetime cu·ªëi
            df_vn.drop_duplicates(subset=["Tinh_thanh", "Datetime"], inplace=True)
            df_vn["Datetime"] = pd.to_datetime(df_vn["Datetime"], errors="coerce")
            df_vn.dropna(subset=["Datetime"], inplace=True)
            df_vn.sort_values(["Tinh_thanh", "Datetime"], inplace=True)

            # S·∫Øp x·∫øp l·∫°i c·ªôt: Tinh_thanh | M√£ th·ªùi ti·∫øt | M√£ th·ªùi ti·∫øt ng√†y | ... | Datetime
            special_cols = ["M√£ th·ªùi ti·∫øt", "M√£ th·ªùi ti·∫øt ng√†y"]
            cols = ["Tinh_thanh"]
            for sc in special_cols:
                if sc in df_vn.columns:
                    cols.append(sc)

            middle_cols = [
                c for c in df_vn.columns
                if c not in (["Tinh_thanh", "Datetime"] + special_cols)
            ]

            cols += middle_cols
            cols.append("Datetime")
            df_vn = df_vn[cols]

            df_vn.to_csv(out_vn_path, index=False, encoding="utf-8-sig")
            print(f"  ‚úî ƒê√£ c·∫≠p nh·∫≠t file t·ªïng: {out_vn_path} (t·ªïng {len(df_vn)} d√≤ng)")
        else:
            print(f"  ‚úñ Kh√¥ng c√≥ d·ªØ li·ªáu m·ªõi cho {tinh}, kh√¥ng update file t·ªïng.")

        # delay gi·ªØa c√°c t·ªânh: d√†i ƒë·ªÉ gi·∫£m 429 g·∫ßn nh∆∞ h·∫øt
        wait_province = random.uniform(40, 80)
        print(f"  Ngh·ªâ {int(wait_province)}s tr∆∞·ªõc khi sang t·ªânh ti·∫øp theo...")
        time.sleep(wait_province)

except RuntimeError as e:
    if "RATE_LIMIT_HARD" in str(e):
        print("\n‚õî CH∆Ø∆†NG TR√åNH D·ª™NG DO RATE_LIMIT_HARD. H√ÉY CH·∫†Y L·∫†I SAU √çT L√ÇU.")
    else:
        print(f"\n‚õî D·ª™NG DO L·ªñI KH√ÅC: {e}")
else:
    print("\nüéâ HO√ÄN T·∫§T CRAWL TO√ÄN B·ªò VI·ªÜT NAM!")
    if not df_vn.empty:
        print(f"T·ªïng s·ªë d√≤ng cu·ªëi c√πng: {len(df_vn)}")
